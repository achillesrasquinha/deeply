{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692d1598",
   "metadata": {},
   "source": [
    "# Variational AutoEncoders (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3966a9d",
   "metadata": {},
   "source": [
    "A model skeleton for Variational Auto-Encoders (VAE). It was first introduced in the paper titled *[Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)* by Diederik P Kingma and Max Welling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d8db7",
   "metadata": {},
   "source": [
    "> In machine learning, a variational autoencoder, also known as VAE, is the artificial neural network architecture belonging to the families of probabilistic graphical models and variational Bayesian methods.\n",
    "\n",
    "> It is often associated with the autoencoder model because of its architectural affinity, but there are significant differences both in the goal and in the mathematical formulation. Variational autoencoders are meant to compress the input information into a constrained multivariate latent distribution (encoding) to reconstruct it as accurately as possible (decoding). Although this type of model was initially designed for unsupervised learning, its effectiveness has been proven in other domains of machine learning such as semi-supervised learning or supervised learning. - *WikiPedia*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f21cb1e",
   "metadata": {},
   "source": [
    "## usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a2d0a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeply\n",
    "\n",
    "vae = deeply.hub(\"vae\", x = 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064dac58",
   "metadata": {},
   "source": [
    "## example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259828a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import deeply.datasets as dd\n",
    "\n",
    "config = dict(batch_size = 128, epochs = 10)\n",
    "\n",
    "def normalize(image, label):\n",
    "    shape    = tf.shape(image)\n",
    "    prod     = tf.reduce_prod(shape)\n",
    "    reshaped = tf.reshape(image, shape = (28 * 28,))\n",
    "    \n",
    "    return tf.cast(reshaped, tf.float32) / 255\n",
    "\n",
    "def preprocess_ds(ds):\n",
    "    return ds\\\n",
    "        .map(normalize)\\\n",
    "        .cache()\\\n",
    "        .shuffle(dd.length(ds))\\\n",
    "        .batch(config[\"batch_size\"])\\\n",
    "\n",
    "mnist  = dd.load(\"mnist\", as_supervised = True)\n",
    "(train, val), test = map(preprocess_ds, dd.split(mnist[\"train\"], splits = (.8, .2))), mnist[\"test\"]\n",
    "\n",
    "vae.fit(train, **config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
