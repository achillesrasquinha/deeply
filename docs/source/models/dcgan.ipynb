{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3539db47",
   "metadata": {},
   "source": [
    "# Deep Convolutional Generative Adversarial Networks (DCGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4471ba2",
   "metadata": {},
   "source": [
    "A model skeleton for Generative Adversarial Networks (GAN)s. It was first introduced in the paper titled *[Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)* by Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc037db5",
   "metadata": {},
   "source": [
    "> A generative adversarial network (GAN) is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in June 2014. Two neural networks contest with each other in a game (in the form of a zero-sum game, where one agent's gain is another agent's loss).\n",
    "\n",
    "> Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proved useful for semi-supervised learning, fully supervised learning, and reinforcement learning.\n",
    "\n",
    "> The core idea of a GAN is based on the \"indirect\" training through the discriminator, another neural network that can tell how \"realistic\" the input seems, which itself is also being updated dynamically. This means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner.\n",
    "\n",
    "> GANs are similar to mimicry in evolutionary biology, with an evolutionary arms race between both networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ad94ee",
   "metadata": {},
   "source": [
    "## usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e1ee73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class norm:\n",
    "    def fit_transform(self, image):\n",
    "        return (tf.cast(image, tf.float32) - 127.5) / 127.5\n",
    "    \n",
    "    def inverse_transform(self, image):\n",
    "        return (tf.cast(image, tf.float32) * 127.5) + 127.5\n",
    "\n",
    "scaler = norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0e7c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-24 17:14:14.503562: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-24 17:14:14.503582: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-09-24 17:14:16.937646: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-24 17:14:16.937665: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-24 17:14:16.937681: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (test2-cell-collective): /proc/driver/nvidia/version does not exist\n",
      "2022-09-24 17:14:16.937833: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "deeply | 2022-09-24 17:14:17,020 | INFO | Building decoder...\n",
      "deeply | 2022-09-24 17:14:17,103 | INFO | Compiling model dcgan...\n"
     ]
    }
   ],
   "source": [
    "import deeply\n",
    "gan = deeply.hub(\"dcgan\", x = 28, decoder_batch_norm = True, encoder_dropout_rate = 0.3,\n",
    "                 encoder_layer_growth_rate = 2,\n",
    "                 init_decoder_units = 256, decoder_layer_growth_rate = 0.5, kernel_size = 5,\n",
    "                 decoder_strides  = [1, 2], scaler = scaler,\n",
    "                 final_activation = \"tanh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976cd6ac",
   "metadata": {},
   "source": [
    "## example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f1ae69c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import deeply.datasets as dd\n",
    "from deeply.plots import imgplot as imgplt, mplt\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "mnist  = dd.load(\"mnist\", shuffle_files = True, as_supervised = True)\n",
    "\n",
    "mapper = lambda image, label: scaler.fit_transform(image)\n",
    "\n",
    "config = dict(batch_size = 256,\n",
    "              epochs = 50)\n",
    "\n",
    "data   = mnist[\"train\"].map(mapper)\n",
    "batch  = data.batch(config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1b5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = gan.generator.input_shape[1]\n",
    "sample = tf.random.normal([64, noise_dim])\n",
    "\n",
    "def plot_img(model):\n",
    "    imgs = model.generator(sample, training = False)\n",
    "    imgs = scaler.inverse_transform(imgs)\n",
    "    imgs = tf.cast(imgs, tf.uint8)\n",
    "    imgplt(imgs, to_file = \"gan.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efe7858",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015326499938964844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 17,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 50,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f19d5dbd844b3e84b844637a3147bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.07280445098876953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 17,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 235,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e82a7f28e6c42668e1b7431986b0253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helikarlab/dev/deeply/src/deeply/plots.py:113: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = pplt.subplots(n_plots, 1, sharex = True)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02304363250732422,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 17,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 235,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c750d297f624948a8545b5a90e55a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02164769172668457,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 17,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 235,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987359610dc94a298eac55ea48ba1644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for e in trange(config[\"epochs\"]):\n",
    "    for b in tqdm(batch):\n",
    "        history = gan.fit(b, checkpoint_path = \"gan.h5d5\")\n",
    "    plot_img(gan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
