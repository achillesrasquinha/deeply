{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3539db47",
   "metadata": {},
   "source": [
    "# Deep Convolutional Conditional Generative Adversarial Networks (DCCGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4471ba2",
   "metadata": {},
   "source": [
    "A model skeleton for Conditional Generative Adversarial Networks (GAN)s. It was first introduced in the paper titled [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784) by Mehdi Mirza and Simon Osindero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc7b34",
   "metadata": {},
   "source": [
    "> The model is a variant of the original GAN model, where the generator and discriminator are modified to take an additional input, the class label, which is a one-hot vector. This label is incorporated in both the generator and discriminator, such that the generated images are conditioned on the input label. The model can be used to generate images of a specific class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ad94ee",
   "metadata": {},
   "source": [
    "## usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d0e7c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-25 02:02:24.851161: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-25 02:02:27.536542: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import deeply\n",
    "import deeply.datasets as dd\n",
    "\n",
    "mnist, info = dd.load(\"mnist\", shuffle_files = True, as_supervised = True, with_info = True)\n",
    "features    = info.features\n",
    "image_shape = features[\"image\"].shape\n",
    "n_classes   = features[\"label\"].num_classes\n",
    "\n",
    "gan = deeply.hub(\"dcgan\", input_shape = image_shape, n_classes = n_classes,\n",
    "                 decoder_batch_norm = True, encoder_dropout_rate = 0.3,\n",
    "                 encoder_layer_growth_rate = 2, init_decoder_units = 256,\n",
    "                 decoder_layer_growth_rate = 0.5, kernel_size = 5,\n",
    "                 decoder_strides = [1, 2], final_activation = \"tanh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976cd6ac",
   "metadata": {},
   "source": [
    "## example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f1ae69c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from deeply.plots import imgplot as imgplt, mplt\n",
    "from deeply.transformers.scaler import image_scaler\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "def mapper(image, label):\n",
    "    return image_scaler.fit_transform(image), tf.one_hot(label, n_classes)\n",
    "\n",
    "config = dict(batch_size = 256,\n",
    "              epochs = 50)\n",
    "\n",
    "data   = mnist[\"train\"].map(mapper)\n",
    "batch  = data.batch(config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b6c824",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0233156681060791,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 13,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 50,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cde8ece3fe54a58b22efc1db08f6e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 28, 28, 11) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 11), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (32, 28, 28, 1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 28, 28, 11) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 11), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (32, 28, 28, 1).\n",
      "2022-09-25 02:02:28.264360: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/helikarlab/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/helikarlab/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/helikarlab/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/helikarlab/dev/deeply/src/deeply/model/gan.py\", line 99, in train_step\n        loss_generator, loss_discriminator = self.compute_loss(data)\n    File \"/home/helikarlab/dev/deeply/src/deeply/model/gan.py\", line 86, in compute_loss\n        real_output      = self.discriminator(data, training = True)\n    File \"/home/helikarlab/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filebhdohy6j.py\", line 25, in tf__call\n        ag__.for_stmt(ag__.converted_call(ag__.ld(range), (ag__.ld(self).width,), None, fscope), None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'i'})\n    File \"/tmp/__autograph_generated_filebhdohy6j.py\", line 22, in loop_body\n        x = ag__.converted_call(ag__.ld(self).layers[ag__.ld(i)], (ag__.ld(x),), dict(training=ag__.ld(training)), fscope)\n\n    ValueError: Exception encountered when calling layer \"conv_block\" \"                 f\"(type ConvBlock).\n    \n    in user code:\n    \n        File \"/home/helikarlab/dev/deeply/src/deeply/model/layer.py\", line 101, in call  *\n            x = self.layers[i](x, training = training)\n        File \"/home/helikarlab/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/home/helikarlab/.venv/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 277, in assert_input_compatibility\n            raise ValueError(\n    \n        ValueError: Input 0 of layer \"conv2d\" is incompatible with the layer: expected axis -1 of input shape to have value 11, but received input with shape (32, 28, 28, 1)\n    \n    \n    Call arguments received by layer \"conv_block\" \"                 f\"(type ConvBlock):\n      • inputs=tf.Tensor(shape=(32, 28, 28, 1), dtype=float32)\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m trange(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m----> 5\u001b[0m         history \u001b[38;5;241m=\u001b[39m \u001b[43mgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgan.h5d5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     plot_img_samples(gan)\n",
      "File \u001b[0;32m~/dev/deeply/src/deeply/model/gan.py:50\u001b[0m, in \u001b[0;36mGANModel.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# kwargs = update_kwargs(kwargs, {\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m#     \"callbacks\": {\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m#     }\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# })\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     super_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(GANModel, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msuper_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/deeply/src/deeply/model/base.py:34\u001b[0m, in \u001b[0;36mBaseModel.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m get_fit_args_kwargs(\u001b[38;5;28mself\u001b[39m, args, kwargs, custom \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\n\u001b[1;32m     31\u001b[0m })\n\u001b[1;32m     33\u001b[0m super_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(BaseModel, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msuper_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filev6uef72k.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/deeply/src/deeply/model/gan.py:99\u001b[0m, in \u001b[0;36mGANModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     96\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m generator_tape, tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m discriminator_tape:\n\u001b[0;32m---> 99\u001b[0m     loss_generator, loss_discriminator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m generator_gradients \u001b[38;5;241m=\u001b[39m generator_tape\u001b[38;5;241m.\u001b[39mgradient(loss_generator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[1;32m    102\u001b[0m discriminator_gradients \u001b[38;5;241m=\u001b[39m discriminator_tape\u001b[38;5;241m.\u001b[39mgradient(loss_discriminator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[0;32m~/dev/deeply/src/deeply/model/gan.py:86\u001b[0m, in \u001b[0;36mGANModel.compute_loss\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     82\u001b[0m noise \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(shape \u001b[38;5;241m=\u001b[39m (batch_size, noise_dim))\n\u001b[1;32m     84\u001b[0m generated_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(noise, training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 86\u001b[0m real_output      \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m fake_output      \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator(generated_output, training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     89\u001b[0m loss_generator     \u001b[38;5;241m=\u001b[39m generator_loss(fake_output)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filebhdohy6j.py:25\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mactivations[ag__\u001b[38;5;241m.\u001b[39mld(i)], (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     24\u001b[0m i \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filebhdohy6j.py:22\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m x\n\u001b[1;32m     21\u001b[0m i \u001b[38;5;241m=\u001b[39m itr\n\u001b[0;32m---> 22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mactivations[ag__\u001b[38;5;241m.\u001b[39mld(i)], (ag__\u001b[38;5;241m.\u001b[39mld(x),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/helikarlab/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/helikarlab/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/helikarlab/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/helikarlab/dev/deeply/src/deeply/model/gan.py\", line 99, in train_step\n        loss_generator, loss_discriminator = self.compute_loss(data)\n    File \"/home/helikarlab/dev/deeply/src/deeply/model/gan.py\", line 86, in compute_loss\n        real_output      = self.discriminator(data, training = True)\n    File \"/home/helikarlab/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filebhdohy6j.py\", line 25, in tf__call\n        ag__.for_stmt(ag__.converted_call(ag__.ld(range), (ag__.ld(self).width,), None, fscope), None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'i'})\n    File \"/tmp/__autograph_generated_filebhdohy6j.py\", line 22, in loop_body\n        x = ag__.converted_call(ag__.ld(self).layers[ag__.ld(i)], (ag__.ld(x),), dict(training=ag__.ld(training)), fscope)\n\n    ValueError: Exception encountered when calling layer \"conv_block\" \"                 f\"(type ConvBlock).\n    \n    in user code:\n    \n        File \"/home/helikarlab/dev/deeply/src/deeply/model/layer.py\", line 101, in call  *\n            x = self.layers[i](x, training = training)\n        File \"/home/helikarlab/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/home/helikarlab/.venv/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 277, in assert_input_compatibility\n            raise ValueError(\n    \n        ValueError: Input 0 of layer \"conv2d\" is incompatible with the layer: expected axis -1 of input shape to have value 11, but received input with shape (32, 28, 28, 1)\n    \n    \n    Call arguments received by layer \"conv_block\" \"                 f\"(type ConvBlock):\n      • inputs=tf.Tensor(shape=(32, 28, 28, 1), dtype=float32)\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "from deeply.model.gan import plot_img_samples\n",
    "\n",
    "for e in trange(config[\"epochs\"]):\n",
    "    for b in batch:\n",
    "        history = gan.fit(b, checkpoint_path = \"gan.h5d5\")\n",
    "    plot_img_samples(gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c76c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
